{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RobotEnv import RobotEnv\n",
    "import random\n",
    "env = RobotEnv(screen_width=400, screen_height=400)\n",
    "# size of obs\n",
    "observation_length = env.observation_space.shape[0]\n",
    "action_length = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class SelectOutput(torch.nn.Module):\n",
    "    def forward(self, inputs):\n",
    "        outputs, (hidden, cell) = inputs\n",
    "        return outputs\n",
    "\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(observation_length, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.LSTM(64, 64, batch_first=True),\n",
    "    SelectOutput(),  # 添加自定义层\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, action_length),\n",
    ")\n",
    "model_delay = torch.nn.Sequential(\n",
    "    torch.nn.Linear(observation_length, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.LSTM(64, 64, batch_first=True),\n",
    "    SelectOutput(),  # 添加自定义层\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, action_length),\n",
    ")\n",
    "# 复制参数\n",
    "model_delay.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单局游戏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "from utils import Controller,Pool\n",
    "controller = Controller(model, env)\n",
    "pool = Pool(controller)\n",
    "# pool.update()\n",
    "# pool.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\2024Spring\\毕业设计\\code\\RL\\DRQN.ipynb 单元格 8\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/DRQN.ipynb#X10sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m             writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m.\u001b[39mitem(), global_step\u001b[39m=\u001b[39mn_step)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/DRQN.ipynb#X10sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     writer\u001b[39m.\u001b[39mclose()  \u001b[39m# 训练结束后关闭writer\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/DRQN.ipynb#X10sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m train()\n",
      "\u001b[1;32me:\\2024Spring\\毕业设计\\code\\RL\\DRQN.ipynb 单元格 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/DRQN.ipynb#X10sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(value, target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/DRQN.ipynb#X10sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/DRQN.ipynb#X10sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/DRQN.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/DRQN.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# 复制参数\u001b[39;00m\n",
      "File \u001b[1;32me:\\Miniconda3\\envs\\RL\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32me:\\Miniconda3\\envs\\RL\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32me:\\Miniconda3\\envs\\RL\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     adam(\n\u001b[0;32m    167\u001b[0m         params_with_grad,\n\u001b[0;32m    168\u001b[0m         grads,\n\u001b[0;32m    169\u001b[0m         exp_avgs,\n\u001b[0;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    172\u001b[0m         state_steps,\n\u001b[0;32m    173\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m         has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[0;32m    175\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    176\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    177\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    178\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    179\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    180\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    181\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    182\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    183\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    184\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    185\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    186\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32me:\\Miniconda3\\envs\\RL\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m func(params,\n\u001b[0;32m    317\u001b[0m      grads,\n\u001b[0;32m    318\u001b[0m      exp_avgs,\n\u001b[0;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    321\u001b[0m      state_steps,\n\u001b[0;32m    322\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    323\u001b[0m      has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[0;32m    324\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    325\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    326\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    327\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    328\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    329\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    330\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    331\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    332\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    333\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32me:\\Miniconda3\\envs\\RL\\Lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    388\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[0;32m    390\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m exp_avg\u001b[39m.\u001b[39;49mlerp_(grad, \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[0;32m    392\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    394\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 训练\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    # 共训练n_step次\n",
    "    n_step = 0\n",
    "    log_interval = 100_000\n",
    "    last_log_step = 0\n",
    "    writer = SummaryWriter(\"./logs/DRQN_logs\")\n",
    "    # 如果checkpoint存在，则加载checkpoint\n",
    "    if os.path.exists(\"checkpoint/DRQN.pth\"):\n",
    "        model.load_state_dict(torch.load(\"checkpoint/DRQN.pth\"))\n",
    "        model_delay.load_state_dict(torch.load(\"checkpoint/DRQN.pth\"))\n",
    "    if os.path.exists(\"checkpoint/DRQN_logs.json\"):\n",
    "        with open('checkpoint/DRQN_logs.json', 'r') as f:\n",
    "            log_data = np.array(json.load(f))\n",
    "            n_step = log_data[:, 1][-1]\n",
    "            last_log_step = n_step\n",
    "            for row in log_data:\n",
    "                writer.add_scalar('Step', row[1], global_step=row[1])\n",
    "                writer.add_scalar('Test Result', row[2], global_step=row[1])\n",
    "    while n_step < 100_000_000:\n",
    "        n_step += pool.update()\n",
    "        # print(f\"n_step:{n_step}\")\n",
    "        # 每次更新数据后,训练N次\n",
    "        for i in range(200):\n",
    "            # print(f\"i:{i}\")\n",
    "            # 采样N条数据\n",
    "            state, action, reward, next_state, terminated = pool.sample()\n",
    "\n",
    "            # 计算value\n",
    "            value = model(state).gather(dim=1, index=action)\n",
    "\n",
    "            # 计算target\n",
    "            with torch.no_grad():\n",
    "                # 使用原模型计算动作,使用延迟模型计算target,进一步缓解自举\n",
    "                next_action = model(next_state).argmax(dim=1, keepdim=True)\n",
    "                target = model_delay(next_state).gather(dim=1,\n",
    "                                                        index=next_action)\n",
    "            target = target * 0.99 * (1 - terminated) + reward\n",
    "\n",
    "            loss = loss_fn(value, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # 复制参数\n",
    "        if (n_step - last_log_step) >= log_interval:\n",
    "            model_delay.load_state_dict(model.state_dict())\n",
    "            test_result = sum([pool.controller.play(mode=\"test\")[-1]\n",
    "                              for _ in range(20)]) / 20\n",
    "            print(f\"step:{n_step},test_result:{test_result}\")\n",
    "\n",
    "            last_log_step = n_step\n",
    "            # 将步数，测试结果和损失写入TensorBoard\n",
    "            writer.add_scalar('Step', n_step, global_step=n_step)\n",
    "            writer.add_scalar('Test Result', test_result, global_step=n_step)\n",
    "            writer.add_scalar('Loss', loss.item(), global_step=n_step)\n",
    "    writer.close()  # 训练结束后关闭writer\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 保存模型参数\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     # 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'n_step': n_step,\n",
    "# }, 'DRQN.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

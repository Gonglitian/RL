{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from RobotEnv import RobotEnv\n",
    "import numpy as np\n",
    "t_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(8, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 4),\n",
    ")\n",
    "env = RobotEnv(400,400)\n",
    "# 玩一局游戏获取轨迹\n",
    "controller = Controller(t_model,env)\n",
    "t_list = []\n",
    "s, _ = env.reset()\n",
    "\n",
    "\n",
    "def get_trajectory(model,env):\n",
    "    t = []\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not (terminated or truncated):\n",
    "        a = model(torch.FloatTensor(s).reshape(\n",
    "            1, 8)).argmax().item()\n",
    "        if random.random() < 0.1:\n",
    "            a = env.action_space.sample()\n",
    "        ns, r, terminated, truncated, _ = env.step(a)\n",
    "        t.append(ns)\n",
    "    env.reset()\n",
    "    return np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取多条轨迹，存储至t_list\n",
    "for i in range(10_0):\n",
    "    t = get_trajectory(t_model,env)\n",
    "    t_list.append(t)\n",
    "len(t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_list = t_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 假设每个状态的大小为8\n",
    "state_size = 8\n",
    "hidden_size = 128\n",
    "\n",
    "# 创建模型、优化器和损失函数\n",
    "model = GRUModel(state_size, hidden_size, state_size)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "#测试GRU输出\n",
    "x = torch.randn(1, 1, state_size)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Trajectory 1, Loss: 22077.948150946624\n",
      "Epoch 1, Trajectory 2, Loss: 5220.521414203293\n",
      "Test Epoch 1, Loss: 8830.32501308439\n",
      "Epoch 2, Trajectory 1, Loss: 7045.558287415887\n",
      "Epoch 2, Trajectory 2, Loss: 2435.2703364834465\n",
      "Test Epoch 2, Loss: 5132.650393686187\n",
      "Epoch 3, Trajectory 1, Loss: 2861.9415237092626\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\2024Spring\\毕业设计\\code\\RL\\state_predict.ipynb 单元格 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/state_predict.ipynb#W4sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m, epoch_loss, global_step\u001b[39m=\u001b[39mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/state_predict.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTest Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/state_predict.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m train()\n",
      "\u001b[1;32me:\\2024Spring\\毕业设计\\code\\RL\\state_predict.ipynb 单元格 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/state_predict.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/state_predict.ipynb#W4sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         t_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/state_predict.ipynb#W4sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/state_predict.ipynb#W4sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     reset_model(model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/2024Spring/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/code/RL/state_predict.ipynb#W4sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m t_loss \u001b[39m/\u001b[39m (m \u001b[39m*\u001b[39m repeat)\n",
      "File \u001b[1;32me:\\Miniconda3\\envs\\RL\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32me:\\Miniconda3\\envs\\RL\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32me:\\Miniconda3\\envs\\RL\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     adam(\n\u001b[0;32m    167\u001b[0m         params_with_grad,\n\u001b[0;32m    168\u001b[0m         grads,\n\u001b[0;32m    169\u001b[0m         exp_avgs,\n\u001b[0;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    172\u001b[0m         state_steps,\n\u001b[0;32m    173\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m         has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[0;32m    175\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    176\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    177\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    178\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    179\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    180\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    181\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    182\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    183\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    184\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    185\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    186\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32me:\\Miniconda3\\envs\\RL\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m func(params,\n\u001b[0;32m    317\u001b[0m      grads,\n\u001b[0;32m    318\u001b[0m      exp_avgs,\n\u001b[0;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    321\u001b[0m      state_steps,\n\u001b[0;32m    322\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    323\u001b[0m      has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[0;32m    324\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    325\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    326\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    327\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    328\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    329\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    330\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    331\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    332\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    333\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32me:\\Miniconda3\\envs\\RL\\Lib\\site-packages\\torch\\optim\\adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    437\u001b[0m         denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    438\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[0;32m    441\u001b[0m     param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n\u001b[0;32m    443\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 训练模型\n",
    "writer = SummaryWriter(\"./logs/state_prediction\")\n",
    "def reset_model(model):\n",
    "    model.hidden = None\n",
    "\n",
    "def test():\n",
    "    loss_sum = 0\n",
    "    for trajectory in t_list:\n",
    "        trajectory = torch.tensor(trajectory, dtype=torch.float32)\n",
    "        m = len(trajectory)\n",
    "        t_loss = 0\n",
    "        for j, state in enumerate(trajectory):\n",
    "            if j == m - 1:\n",
    "                break\n",
    "            s = torch.FloatTensor(state).reshape(1, 1, 8)\n",
    "            next_s = torch.FloatTensor(trajectory[j + 1]).reshape(1, 8)\n",
    "            predict_state = model(s)\n",
    "            t_loss+= loss_fn(predict_state, next_s)\n",
    "        reset_model(model)\n",
    "        avg_loss_t = t_loss.item() / m\n",
    "        loss_sum += avg_loss_t\n",
    "    return loss_sum / len(t_list)\n",
    "\n",
    "def train():\n",
    "    for epoch in range(30):  # 进行100个训练周期\n",
    "        for i,trajectory in enumerate(t_list):\n",
    "            # 将轨迹转换为Tensor\n",
    "            trajectory = torch.tensor(trajectory, dtype=torch.float32)\n",
    "            # 获取输入和目标\n",
    "            m = len(trajectory)\n",
    "            t_loss = 0\n",
    "            repeat = 10\n",
    "            for n in range(repeat):\n",
    "                for j,state in enumerate(trajectory):\n",
    "                    if j == m - 1:\n",
    "                        break\n",
    "                    s = torch.FloatTensor(state).reshape(1, 1, 8)\n",
    "                    next_s = torch.FloatTensor(trajectory[j + 1]).reshape(1, 8)\n",
    "                    predict_state = model(s)\n",
    "                    # 计算损失\n",
    "                    loss = loss_fn(predict_state, next_s)\n",
    "                    # 反向传播和优化\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    t_loss += loss.item()\n",
    "                    optimizer.step()\n",
    "                reset_model(model)\n",
    "            avg_loss = t_loss / (m * repeat)\n",
    "            print(f'Epoch {epoch+1}, Trajectory {i+1}, Loss: {avg_loss}')\n",
    "        # 每个epoch测试一次，记录所有轨迹预测的平均loss\n",
    "        epoch_loss = test()\n",
    "        writer.add_scalar('Loss', epoch_loss, global_step=epoch+1)\n",
    "        print(f'Test Epoch {epoch+1}, Loss: {epoch_loss}')\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save \n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
